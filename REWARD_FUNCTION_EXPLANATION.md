# Reward Function è¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†è§£é‡Š `/data/OpenTau/src/opentau/policies/value/reward.py` ä¸­çš„å¥–åŠ±å‡½æ•°å®ç°ã€‚

---

## ğŸ“‹ æ¦‚è¿°

è¿™ä¸ªæ¨¡å—å®ç°äº†**ç¨€ç–å¥–åŠ±å‡½æ•°ï¼ˆSparse Reward Functionï¼‰**ï¼Œç”¨äºï¼š
1. **Value Functionè®­ç»ƒ**ï¼šå°†returnç¦»æ•£åŒ–ä¸ºbinsç”¨äºåˆ†ç±»è®­ç»ƒ
2. **Advantageè®¡ç®—**ï¼šè®¡ç®—n-step returnç”¨äºadvantageä¼°è®¡

---

## ğŸ¯ å¥–åŠ±å‡½æ•°è®¾è®¡

### å¥–åŠ±å‡½æ•°å…¬å¼

æ ¹æ®RECAPæ–‡æ¡£ï¼Œå¥–åŠ±å‡½æ•°å®šä¹‰ä¸ºï¼š

```
r_t = {
    0          if t = T and success
    -C_fail    if t = T and failure
    -1         otherwise
}
```

å…¶ä¸­ï¼š
- `t`: å½“å‰æ—¶é—´æ­¥
- `T`: episodeçš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥
- `C_fail`: å¤±è´¥episodeçš„å¤§è´Ÿå¸¸æ•°ï¼ˆé»˜è®¤-1000.0ï¼‰
- `success`: episodeæ˜¯å¦æˆåŠŸ

### è®¾è®¡ç†å¿µ

è¿™æ˜¯ä¸€ä¸ª**æ—¶é—´æƒ©ç½šå¥–åŠ±å‡½æ•°**ï¼š
- **æ¯æ­¥æƒ©ç½š**: æ¯æ‰§è¡Œä¸€æ­¥ï¼Œå¥–åŠ±-1ï¼ˆé¼“åŠ±å¿«é€Ÿå®Œæˆä»»åŠ¡ï¼‰
- **æˆåŠŸå¥–åŠ±**: æˆåŠŸå®Œæˆæ—¶ï¼Œæœ€åä¸€æ­¥å¥–åŠ±ä¸º0ï¼ˆæ— é¢å¤–æƒ©ç½šï¼‰
- **å¤±è´¥æƒ©ç½š**: å¤±è´¥æ—¶ï¼Œæœ€åä¸€æ­¥é¢å¤–æƒ©ç½š `-C_fail`ï¼ˆå¼ºçƒˆæƒ©ç½šå¤±è´¥ï¼‰

**ç›®æ ‡**: Value functioné¢„æµ‹ä»å½“å‰çŠ¶æ€åˆ°æˆåŠŸçš„**å‰©ä½™æ­¥æ•°**ï¼ˆè´Ÿæ•°ï¼‰ï¼Œæˆ–å¤±è´¥æ—¶çš„**å¤§è´Ÿå€¼**ã€‚

---

## ğŸ“Š å‡½æ•°1: `calculate_return_bins_with_equal_width`

### åŠŸèƒ½
è®¡ç®—ä»å½“å‰çŠ¶æ€åˆ°episodeç»“æŸçš„**ç´¯ç§¯return**ï¼Œå¹¶å°†å…¶**ç¦»æ•£åŒ–ä¸ºbins**ç”¨äºValue Functionçš„åˆ†ç±»è®­ç»ƒã€‚

### å‚æ•°è¯´æ˜

```python
def calculate_return_bins_with_equal_width(
    success: bool,              # episodeæ˜¯å¦æˆåŠŸ
    b: int,                     # binçš„æ•°é‡ï¼ˆé€šå¸¸201ï¼‰
    episode_end_idx: int,       # episodeç»“æŸç´¢å¼•ï¼ˆä¸åŒ…å«æœ€åä¸€æ­¥ï¼‰
    reward_normalizer: int,     # å½’ä¸€åŒ–å› å­ï¼ˆæœ€å¤§episodeé•¿åº¦ï¼‰
    current_idx: int,           # å½“å‰æ—¶é—´æ­¥ç´¢å¼•
    c_neg: float = -100.0,      # å¤±è´¥æƒ©ç½šå¸¸æ•°
) -> tuple[int, float]:
```

### è®¡ç®—æ­¥éª¤

#### æ­¥éª¤1: è®¡ç®—åŸºç¡€returnå€¼
```python
return_value = current_idx - episode_end_idx + 1
```

**å«ä¹‰**: è®¡ç®—ä»å½“å‰æ­¥åˆ°episodeç»“æŸçš„æ­¥æ•°ï¼ˆè´Ÿæ•°ï¼‰

**ç¤ºä¾‹**:
- å¦‚æœepisodeåœ¨ç¬¬100æ­¥ç»“æŸï¼Œå½“å‰åœ¨ç¬¬50æ­¥
- `return_value = 50 - 100 + 1 = -49`
- è¡¨ç¤ºè¿˜éœ€è¦49æ­¥æ‰èƒ½å®Œæˆï¼ˆå¦‚æœæˆåŠŸï¼‰

#### æ­¥éª¤2: æ·»åŠ å¤±è´¥æƒ©ç½š
```python
if not success:
    return_value += c_neg
```

**å«ä¹‰**: å¦‚æœepisodeå¤±è´¥ï¼Œæ·»åŠ å¤§è´Ÿæƒ©ç½š

**ç¤ºä¾‹**:
- å¦‚æœå¤±è´¥ä¸” `c_neg = -1000`
- `return_value = -49 + (-1000) = -1049`
- å¤±è´¥episodeçš„returnä¼šæ˜¯éå¸¸å¤§çš„è´Ÿæ•°

#### æ­¥éª¤3: å½’ä¸€åŒ–åˆ°[-1, 0)èŒƒå›´
```python
return_normalized = return_value / reward_normalizer
```

**å«ä¹‰**: å°†returnå€¼å½’ä¸€åŒ–åˆ°[-1, 0)åŒºé—´

**ç¤ºä¾‹**:
- å¦‚æœ `reward_normalizer = 400`
- `return_normalized = -49 / 400 = -0.1225`
- `return_normalized = -1049 / 400 = -2.6225` (ä¼šè¢«clampåˆ°-1)

#### æ­¥éª¤4: æ˜ å°„åˆ°binç´¢å¼•
```python
bin_idx = int((return_normalized + 1) * (b - 1))
```

**å«ä¹‰**: å°†å½’ä¸€åŒ–çš„returnå€¼æ˜ å°„åˆ°[0, b-1]çš„binç´¢å¼•

**æ˜ å°„å…¬å¼**:
- `[-1, 0)` â†’ `[0, b-1]`
- çº¿æ€§æ˜ å°„: `bin_idx = (return_normalized + 1) * (b - 1)`

**ç¤ºä¾‹** (b=201):
- `return_normalized = -0.1225` â†’ `bin_idx = int((-0.1225 + 1) * 200) = int(175.5) = 175`
- `return_normalized = -1.0` â†’ `bin_idx = int((-1.0 + 1) * 200) = 0`
- `return_normalized = -0.0` â†’ `bin_idx = int((0.0 + 1) * 200) = 200` (ä½†å®é™…ä¸ä¼šè¾¾åˆ°0)

### è¿”å›å€¼

```python
return bin_idx, return_normalized
```

- `bin_idx`: binç´¢å¼• [0, b-1]ï¼Œç”¨äºåˆ†ç±»è®­ç»ƒ
- `return_normalized`: å½’ä¸€åŒ–çš„è¿ç»­returnå€¼ [-1, 0)ï¼Œç”¨äºè¾…åŠ©æŸå¤±

### ä½¿ç”¨åœºæ™¯

**åœ¨æ•°æ®é›†åŠ è½½æ—¶ä½¿ç”¨** (`lerobot_dataset.py`):
```python
item["return_bin_idx"], item["return_continuous"] = calculate_return_bins_with_equal_width(
    success,
    self.cfg.policy.reward_config.number_of_bins,  # 201
    ep_end,
    self.cfg.policy.reward_config.reward_normalizer,  # 400
    idx,
    self.cfg.policy.reward_config.C_neg,  # -1000.0
)
```

**ç”¨é€”**:
- `return_bin_idx`: ä½œä¸ºåˆ†ç±»æ ‡ç­¾ï¼Œç”¨äºCross-Entropy Loss
- `return_continuous`: ç”¨äºL1 Lossï¼ˆè¾…åŠ©æŸå¤±ï¼‰

---

## ğŸ“ˆ å‡½æ•°2: `calculate_n_step_return`

### åŠŸèƒ½
è®¡ç®—**n-step return**ï¼Œç”¨äºadvantageè®¡ç®—ã€‚è¿™æ˜¯ä»å½“å‰çŠ¶æ€å‘å‰çœ‹Næ­¥çš„ç´¯ç§¯å¥–åŠ±ã€‚

### å‚æ•°è¯´æ˜

```python
def calculate_n_step_return(
    success: bool,              # episodeæ˜¯å¦æˆåŠŸ
    n_steps_look_ahead: int,    # å‘å‰çœ‹çš„æ­¥æ•°ï¼ˆé€šå¸¸50ï¼‰
    episode_end_idx: int,       # episodeç»“æŸç´¢å¼•
    reward_normalizer: int,     # å½’ä¸€åŒ–å› å­
    current_idx: int,           # å½“å‰æ—¶é—´æ­¥ç´¢å¼•
    c_neg: float = -100.0,      # å¤±è´¥æƒ©ç½šå¸¸æ•°
) -> float:
```

### è®¡ç®—æ­¥éª¤

#### æ­¥éª¤1: è®¡ç®—n-stepå†…çš„returnå€¼
```python
return_value = max(current_idx - episode_end_idx + 1, -1 * n_steps_look_ahead)
```

**å«ä¹‰**: 
- è®¡ç®—åˆ°episodeç»“æŸçš„æ­¥æ•°ï¼Œä½†**æœ€å¤šåªçœ‹næ­¥**
- å¦‚æœè·ç¦»ç»“æŸè¶…è¿‡næ­¥ï¼Œåªè®¡ç®—næ­¥çš„æƒ©ç½š

**ç¤ºä¾‹**:
- å½“å‰åœ¨ç¬¬50æ­¥ï¼Œepisodeåœ¨ç¬¬100æ­¥ç»“æŸï¼Œn=50
- `return_value = max(50 - 100 + 1, -50) = max(-49, -50) = -49`
- å½“å‰åœ¨ç¬¬10æ­¥ï¼Œepisodeåœ¨ç¬¬100æ­¥ç»“æŸï¼Œn=50
- `return_value = max(10 - 100 + 1, -50) = max(-89, -50) = -50` (é™åˆ¶åœ¨-næ­¥)

#### æ­¥éª¤2: æ·»åŠ å¤±è´¥æƒ©ç½šï¼ˆå¦‚æœnæ­¥å†…åˆ°è¾¾å¤±è´¥ï¼‰
```python
if not success and current_idx + n_steps_look_ahead >= episode_end_idx:
    return_value += c_neg
```

**å«ä¹‰**: 
- å¦‚æœepisodeå¤±è´¥ï¼Œ**ä¸”**åœ¨næ­¥å†…ä¼šåˆ°è¾¾å¤±è´¥çŠ¶æ€
- åˆ™æ·»åŠ å¤±è´¥æƒ©ç½š

**é€»è¾‘**:
- `current_idx + n_steps_look_ahead >= episode_end_idx` è¡¨ç¤ºåœ¨næ­¥å†…ä¼šåˆ°è¾¾episodeç»“æŸ
- å¦‚æœå¤±è´¥ï¼Œè¯´æ˜åœ¨næ­¥å†…ä¼šå¤±è´¥ï¼Œéœ€è¦æ·»åŠ æƒ©ç½š

**ç¤ºä¾‹**:
- å½“å‰åœ¨ç¬¬95æ­¥ï¼Œepisodeåœ¨ç¬¬100æ­¥ç»“æŸï¼ˆå¤±è´¥ï¼‰ï¼Œn=50
- `95 + 50 >= 100` â†’ Trueï¼Œä¸”å¤±è´¥
- `return_value = -5 + (-1000) = -1005`

#### æ­¥éª¤3: å½’ä¸€åŒ–
```python
return_normalized = return_value / reward_normalizer
```

**å«ä¹‰**: å½’ä¸€åŒ–åˆ°[-1, 0)èŒƒå›´

### è¿”å›å€¼

```python
return return_normalized  # floatå€¼ï¼ŒèŒƒå›´[-1, 0)
```

### ä½¿ç”¨åœºæ™¯

**åœ¨è®¡ç®—advantageæ—¶ä½¿ç”¨** (`get_advantage_and_percentiles.py`):
```python
reward = calculate_n_step_return(
    success=success,
    n_steps_look_ahead=cfg.policy.reward_config.N_steps_look_ahead,  # 50
    episode_end_idx=episode_end_idx,
    max_episode_length=cfg.policy.reward_config.reward_normalizer,  # 400
    current_idx=current_idx,
    c_neg=cfg.policy.reward_config.C_neg,  # -1000.0
)
```

**ç”¨é€”**: ç”¨äºè®¡ç®—advantage
```
Advantage = reward + V(s_{t+N}) - V(s_t)
```

å…¶ä¸­ï¼š
- `reward`: n-step returnï¼ˆè¿™ä¸ªå‡½æ•°çš„è¿”å›å€¼ï¼‰
- `V(s_{t+N})`: Næ­¥åçŠ¶æ€çš„ä»·å€¼
- `V(s_t)`: å½“å‰çŠ¶æ€çš„ä»·å€¼

---

## ğŸ”„ ä¸¤ä¸ªå‡½æ•°çš„åŒºåˆ«

| ç‰¹æ€§ | `calculate_return_bins_with_equal_width` | `calculate_n_step_return` |
|------|------------------------------------------|---------------------------|
| **ç”¨é€”** | Value Functionè®­ç»ƒï¼ˆåˆ†ç±»ï¼‰ | Advantageè®¡ç®— |
| **è¿”å›å€¼** | `(bin_idx, return_normalized)` | `return_normalized` |
| **æ—¶é—´èŒƒå›´** | åˆ°episodeç»“æŸ | å‘å‰çœ‹Næ­¥ |
| **ç¦»æ•£åŒ–** | âœ… æ˜ å°„åˆ°binç´¢å¼• | âŒ åªè¿”å›è¿ç»­å€¼ |
| **ä½¿ç”¨åœºæ™¯** | æ•°æ®é›†é¢„å¤„ç† | è¿è¡Œæ—¶è®¡ç®— |

---

## ğŸ“ å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹1: æˆåŠŸepisode

**åœºæ™¯**:
- Episodeåœ¨ç¬¬100æ­¥æˆåŠŸç»“æŸ
- å½“å‰åœ¨ç¬¬50æ­¥
- `reward_normalizer = 400`, `c_neg = -1000`, `b = 201`, `n = 50`

**`calculate_return_bins_with_equal_width`**:
```python
return_value = 50 - 100 + 1 = -49  # è¿˜éœ€è¦49æ­¥
# success = Trueï¼Œä¸æ·»åŠ æƒ©ç½š
return_normalized = -49 / 400 = -0.1225
bin_idx = int((-0.1225 + 1) * 200) = 175
# è¿”å›: (175, -0.1225)
```

**`calculate_n_step_return`**:
```python
return_value = max(50 - 100 + 1, -50) = max(-49, -50) = -49
# success = Trueï¼Œä¸æ·»åŠ æƒ©ç½š
return_normalized = -49 / 400 = -0.1225
# è¿”å›: -0.1225
```

### ç¤ºä¾‹2: å¤±è´¥episode

**åœºæ™¯**:
- Episodeåœ¨ç¬¬100æ­¥å¤±è´¥ç»“æŸ
- å½“å‰åœ¨ç¬¬50æ­¥
- å‚æ•°åŒä¸Š

**`calculate_return_bins_with_equal_width`**:
```python
return_value = 50 - 100 + 1 = -49
return_value += -1000 = -1049  # æ·»åŠ å¤±è´¥æƒ©ç½š
return_normalized = -1049 / 400 = -2.6225  # ä¼šè¢«clampåˆ°-1
bin_idx = int((-1.0 + 1) * 200) = 0  # æ˜ å°„åˆ°ç¬¬ä¸€ä¸ªbin
# è¿”å›: (0, -1.0)
```

**`calculate_n_step_return`**:
```python
return_value = max(50 - 100 + 1, -50) = -49
# 50 + 50 >= 100 â†’ Trueï¼Œä¸”å¤±è´¥
return_value += -1000 = -1049
return_normalized = -1049 / 400 = -2.6225  # ä¼šè¢«clampåˆ°-1
# è¿”å›: -1.0
```

### ç¤ºä¾‹3: è·ç¦»ç»“æŸå¾ˆè¿œ

**åœºæ™¯**:
- Episodeåœ¨ç¬¬100æ­¥ç»“æŸ
- å½“å‰åœ¨ç¬¬10æ­¥
- `n = 50`

**`calculate_return_bins_with_equal_width`**:
```python
return_value = 10 - 100 + 1 = -89  # è¿˜éœ€è¦89æ­¥
return_normalized = -89 / 400 = -0.2225
bin_idx = int((-0.2225 + 1) * 200) = 155
# è¿”å›: (155, -0.2225)
```

**`calculate_n_step_return`**:
```python
return_value = max(10 - 100 + 1, -50) = max(-89, -50) = -50  # é™åˆ¶åœ¨-næ­¥
return_normalized = -50 / 400 = -0.125
# è¿”å›: -0.125
```

---

## ğŸ“ å…³é”®ç†è§£ç‚¹

1. **ç¨€ç–å¥–åŠ±**: ä¸æ˜¯æ¯æ­¥éƒ½æœ‰æ˜ç¡®çš„å¥–åŠ±ä¿¡å·ï¼Œåªåœ¨episodeç»“æŸæ—¶çŸ¥é“æˆåŠŸ/å¤±è´¥
2. **æ—¶é—´æƒ©ç½š**: æ¯æ­¥-1çš„æƒ©ç½šé¼“åŠ±å¿«é€Ÿå®Œæˆä»»åŠ¡
3. **å¤±è´¥æƒ©ç½š**: å¤§è´Ÿå¸¸æ•° `C_neg` å¼ºçƒˆæƒ©ç½šå¤±è´¥
4. **å½’ä¸€åŒ–**: æ‰€æœ‰returnå€¼å½’ä¸€åŒ–åˆ°[-1, 0)ä¾¿äºè®­ç»ƒ
5. **ç¦»æ•£åŒ–**: å°†è¿ç»­returnç¦»æ•£åŒ–ä¸ºbinsç”¨äºåˆ†ç±»è®­ç»ƒï¼ˆDistributional RLï¼‰
6. **n-step**: advantageè®¡ç®—æ—¶åªå‘å‰çœ‹Næ­¥ï¼Œå¹³è¡¡åå·®å’Œæ–¹å·®

---

## ğŸ”— ç›¸å…³æ–‡ä»¶

- **ä½¿ç”¨ä½ç½®1**: `/data/OpenTau/src/opentau/datasets/lerobot_dataset.py` (ç¬¬1521è¡Œ)
- **ä½¿ç”¨ä½ç½®2**: `/data/OpenTau/src/opentau/scripts/get_advantage_and_percentiles.py` (ç¬¬171è¡Œ)
- **Value Functionè®­ç»ƒ**: `/data/OpenTau/src/opentau/policies/value/modeling_value.py`
- **é…ç½®**: `/data/OpenTau/src/opentau/configs/reward.py`

---

## ğŸ“š å‚è€ƒ

- RECAPè®­ç»ƒæ–‡æ¡£: `/data/OpenTau/docs/source/tutorials/RECAP.rst`
- Distributional RL: C51, QR-DQNç­‰æ–¹æ³•çš„ç¦»æ•£åŒ–æ€æƒ³
